import 'dart:convert';
import 'dart:io';
import 'package:http/http.dart' as http;
import 'package:flutter_dotenv/flutter_dotenv.dart';
import '../../../core/errors/exceptions.dart';
import '../../models/language_prompts.dart';

abstract class OpenAIDataSource {
  Future<String> transcribeAudio(String audioPath);
  Future<String> generateSummary({
    required String transcript,
    required String model,
    required String language,
  });
  Future<String> askQuestion(String question, String context);
}

class OpenAIDataSourceImpl implements OpenAIDataSource {
  final http.Client client;
  final String apiKey;
  final String orgId;
  final bool useMockResponses;
  final String baseUrl = 'https://api.openai.com/v1';
  
  OpenAIDataSourceImpl({http.Client? client, String? apiKey, String? orgId})
      : client = client ?? http.Client(),
        apiKey = apiKey ?? dotenv.env['OPENAI_API_KEY'] ?? 'your_api_key_here',
        orgId = orgId ?? dotenv.env['OPENAI_ORG_ID'] ?? '',
        useMockResponses = dotenv.env['MOCK_AI_RESPONSES']?.toLowerCase() == 'true';
  
  Map<String, String> get _headers => {
    'Authorization': 'Bearer $apiKey',
    'Content-Type': 'application/json',
    if (orgId.isNotEmpty) 'OpenAI-Organization': orgId,
  };
  
  @override
  Future<String> transcribeAudio(String audioPath) async {
    if (useMockResponses) {
      // Mock implementation for development/testing
      await Future.delayed(const Duration(seconds: 2)); // Simulate API call
      return 'This is a mock transcript of the audio recording. In a real implementation, this would be generated by OpenAI Whisper API.';
    }
    
    try {
      // Check if audio file exists
      final audioFile = File(audioPath);
      if (!await audioFile.exists()) {
        throw ServerException('Audio file not found: $audioPath');
      }
      
      // Check file size (OpenAI limit is 25MB)
      final fileSize = await audioFile.length();
      if (fileSize > 25 * 1024 * 1024) {
        throw ServerException('Audio file too large. Maximum size is 25MB.');
      }
      
      // Create multipart request for Whisper API
      final request = http.MultipartRequest(
        'POST',
        Uri.parse('$baseUrl/audio/transcriptions'),
      );
      
      // Add headers
      request.headers.addAll({
        'Authorization': 'Bearer $apiKey',
        if (orgId.isNotEmpty) 'OpenAI-Organization': orgId,
      });
      
      // Add audio file
      request.files.add(await http.MultipartFile.fromPath('file', audioPath));
      
      // Add model parameter
      request.fields['model'] = 'whisper-1';
      request.fields['response_format'] = 'text';
      
      // Send request
      final streamedResponse = await client.send(request);
      final response = await http.Response.fromStream(streamedResponse);
      
      if (response.statusCode == 200) {
        return response.body.trim();
      } else {
        final errorBody = json.decode(response.body);
        throw ServerException('OpenAI API error: ${errorBody['error']?['message'] ?? 'Unknown error'}');
      }
    } catch (e) {
      if (e is ServerException) rethrow;
      throw ServerException('Failed to transcribe audio: $e');
    }
  }
  
  @override
  Future<String> generateSummary({
    required String transcript,
    required String model,
    required String language,
  }) async {
    if (useMockResponses) {
      // Mock implementation for development/testing
      await Future.delayed(const Duration(seconds: 1)); // Simulate API call
      return 'This is a mock summary of the transcript in $language. In a real implementation, this would be generated by OpenAI GPT API.';
    }
    
    try {
      // Get language-specific prompt
      final languagePrompts = _getLanguageSpecificPrompt(language);
      
      final requestBody = {
        'model': model,
        'messages': [
          {
            'role': 'system',
            'content': languagePrompts,
          },
          {
            'role': 'user',
            'content': 'Summarize this transcript: $transcript'
          }
        ],
        'max_tokens': 500,
        'temperature': 0.3,
      };
      
      final response = await client.post(
        Uri.parse('$baseUrl/chat/completions'),
        headers: _headers,
        body: json.encode(requestBody),
      );
      
      if (response.statusCode == 200) {
        final responseBody = json.decode(response.body);
        final summary = responseBody['choices']?[0]?['message']?['content'];
        if (summary != null) {
          return summary.trim();
        } else {
          throw ServerException('Invalid response format from OpenAI API');
        }
      } else {
        final errorBody = json.decode(response.body);
        throw ServerException('OpenAI API error: ${errorBody['error']?['message'] ?? 'Unknown error'}');
      }
    } catch (e) {
      if (e is ServerException) rethrow;
      throw ServerException('Failed to generate summary: $e');
    }
  }
  
  @override
  Future<String> askQuestion(String question, String context) async {
    if (useMockResponses) {
      // Mock implementation for development/testing
      await Future.delayed(const Duration(seconds: 1)); // Simulate API call
      return 'This is a mock answer to your question: "$question". In a real implementation, this would be generated by OpenAI GPT API based on the context provided.';
    }
    
    try {
      final requestBody = {
        'model': 'gpt-3.5-turbo',
        'messages': [
          {
            'role': 'system',
            'content': 'You are a helpful assistant that answers questions based on the provided context from audio recordings. If the context doesn\'t contain enough information to answer the question, please say so. Be concise but informative.'
          },
          {
            'role': 'user',
            'content': 'Context from audio recording:\n$context\n\nQuestion: $question'
          }
        ],
        'max_tokens': 300,
        'temperature': 0.5,
      };
      
      final response = await client.post(
        Uri.parse('$baseUrl/chat/completions'),
        headers: _headers,
        body: json.encode(requestBody),
      );
      
      if (response.statusCode == 200) {
        final responseBody = json.decode(response.body);
        final answer = responseBody['choices']?[0]?['message']?['content'];
        if (answer != null) {
          return answer.trim();
        } else {
          throw ServerException('Invalid response format from OpenAI API');
        }
      } else {
        final errorBody = json.decode(response.body);
        throw ServerException('OpenAI API error: ${errorBody['error']?['message'] ?? 'Unknown error'}');
      }
    } catch (e) {
      if (e is ServerException) rethrow;
      throw ServerException('Failed to get answer: $e');
    }
  }

  /// Get language-specific prompt for summary generation
  String _getLanguageSpecificPrompt(String language) {
    try {
      return LanguagePrompts.getSummaryPrompt(language);
    } catch (e) {
      // Fallback to English prompt if language not supported
      return _getFallbackPrompt();
    }
  }

  /// Fallback English prompt
  String _getFallbackPrompt() {
    return '''
You are an expert at creating structured summaries of educational content, lectures, and discussions.

Create a comprehensive summary in Markdown format with the following structure:
- Clear headings and subheadings using ## and ###
- Bullet points for key information using -
- Bold text for important concepts using **text**
- LaTeX formulas for mathematical expressions using \$formula\$
- Organized sections by topic
- Professional and educational tone

Format example:
## Main Topic
- Key point 1
- Key point 2

### Subsection
**Important concept**: Explanation

**Formula**: \$x^2 + y^2 = z^2\$

Focus on:
1. Main topics and themes
2. Key concepts and definitions
3. Important formulas and calculations
4. Action items or assignments
5. Questions or discussions raised

Make the summary comprehensive but concise, highlighting the most important information for future reference.
''';
  }
}
